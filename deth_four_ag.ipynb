{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6896d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfe616",
   "metadata": {},
   "source": [
    "# Four in a row Agent and Environment\n",
    "\n",
    "This is an implementation of a rational agents for the game four in a row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811a4e2",
   "metadata": {},
   "source": [
    "## 1. Definition of the Environment\n",
    "\n",
    "- States: The environment is in one of the following states $S = (S_0, S_1, ..., S_w)$ with $S_x = (S_{x,0}, S_{x,1}, ..., S_{x,h})$ and $S_{x,y} \\in \\{E, R, Y\\}$.\n",
    "- Actions: The set of  available actions: $a \\in \\{n \\in \\mathbb{N}: (|E \\in S_n| > 0)\\}$ (the agent can place their chip in any non-full column).\n",
    "- Transitions: The transition depends on the opponent's strategy. If we assume the opponent plays randomly then the probability of them picking a viable column is $\\dfrac{1}{nr\\;of\\;valid\\;columns}$\n",
    "\n",
    "The `FourInARowEnv` class has the following methods:\n",
    "- `reset()` resets the environment's state to it's initial state.\n",
    "- `step(action)` processes the action of the agent.\n",
    "- `render()` displays the state using box characters.\n",
    "- `get_state()` returns the current state.\n",
    "- `is_done()` checks if there is a winner of if the board is full.\n",
    "\n",
    "To allow an agent to calculate optimal decisions using model information, these methods are also available:\n",
    "- `get_possible_states()` calculates all possible future states.\n",
    "- `get_reward_for_state(state)` simplified version $R(s)$ of the general reward function: $R(s, a, s')$.\n",
    "- `get_transition_prob(action, new_state, old_state)` $P(s' \\mid s, a)$.\n",
    "- `get_possible_actions(action)` returns the possible actions.\n",
    "- `get_possible_states_after_action(action)` calculates the possible states after an action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19188f",
   "metadata": {},
   "source": [
    "### 1.1 Definition of an agent\n",
    "\n",
    "Agents have access to the environment they play in and what player they play as. An agent has only 1 method named `get_move`. `get_move` is responsible for calculating the optimal action and returning it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec035b",
   "metadata": {},
   "source": [
    "## 2. Environment Creation\n",
    "\n",
    "We are able to create an environment with several parameters:\n",
    "\n",
    "- `yellow_agent` is used to define an agent that will act as the opponent.\n",
    "- `width` the width of the playing field.\n",
    "- `height` the height of the playing field.\n",
    "- `win_conditions` the number of chips in a row required to win.\n",
    "- `first_turn` the player that will play first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12f32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import FourInARowEnv, Players, render_multiple_states, BoxState, FourInARowRenderer\n",
    "from src.agents import FourInARowRandomAgent, FourInARowMinMaxAgent, FourInARowValueIterationAgent, FourInARowSemiRandomAgent\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e3c6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = FourInARowEnv(\n",
    "  yellow_agent  = FourInARowRandomAgent,\n",
    "  width         = 3,\n",
    "  height        = 3,\n",
    "  win_condition = 3,\n",
    "  first_turn    = Players.RED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987af686",
   "metadata": {},
   "source": [
    "### 2.1 Displaying the board\n",
    "\n",
    "The board is displayed using box characters. The letter `Y` represents a yellow chip, and the letter `R` represents a red one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "260dddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐\n",
      "│   │   │   │\n",
      "├───┼───┼───┤\n",
      "│   │   │   │\n",
      "├───┼───┼───┤\n",
      "│ R │   │ Y │\n",
      "└───┴───┴───┘\n"
     ]
    }
   ],
   "source": [
    "red_agent = FourInARowRandomAgent(environment)\n",
    "\n",
    "environment.step(red_agent.get_move())\n",
    "\n",
    "print(environment.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5407e8",
   "metadata": {},
   "source": [
    "## 3. Actions and probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb13f0",
   "metadata": {},
   "source": [
    "### 3.1 Possible states and actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17961dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐\n",
      "│   │   │   │ │   │   │   │ │ R │   │   │ │ R │   │   │ │ R │   │   │ │ R │ Y │   │ │ R │ Y │   │ │ R │   │   │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│   │   │   │ │ Y │   │   │ │ Y │   │   │ │ Y │   │   │ │ Y │ R │   │ │ Y │ R │   │ │ Y │ R │   │ │ Y │ R │   │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│ R │   │   │ │ R │   │   │ │ R │   │   │ │ R │ Y │   │ │ R │ Y │   │ │ R │ Y │   │ │ R │ Y │ R │ │ R │ Y │ Y │\n",
      "└───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘\n",
      " a: [0, 1, 2]  a: [0, 1, 2]  a: [1, 2]     a: [1, 2]     a: [1, 2]     a: [2]        a: [2]        a: [1, 2]   \n",
      " f: False      f: False      f: False      f: False      f: False      f: False      f: True       f: False    \n",
      "\n",
      "┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐\n",
      "│ R │ R │   │ │ R │ R │   │ │ R │ R │ R │ │ R │   │   │ │ R │ Y │   │ │ R │ Y │ R │ │ R │   │ Y │ │ R │ R │ Y │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│ Y │ R │   │ │ Y │ R │ Y │ │ Y │ R │ Y │ │ Y │ R │ R │ │ Y │ R │ R │ │ Y │ R │ R │ │ Y │ R │ R │ │ Y │ R │ R │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│ R │ Y │ Y │ │ R │ Y │ Y │ │ R │ Y │ Y │ │ R │ Y │ Y │ │ R │ Y │ Y │ │ R │ Y │ Y │ │ R │ Y │ Y │ │ R │ Y │ Y │\n",
      "└───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘\n",
      " a: [2]        a: [2]        a: []         a: [1, 2]     a: [2]        a: []         a: [1]        a: []       \n",
      " f: False      f: False      f: True       f: False      f: False      f: True       f: False      f: True     \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "environment.reset()\n",
    "\n",
    "display = render_multiple_states(\n",
    "  states          = environment.get_possible_states()[0:16], \n",
    "  columns         = 8, \n",
    "  additional_info = lambda s: f'a: {environment.get_possible_actions(s)}\\nf: {s.is_finished()}'\n",
    ")\n",
    "\n",
    "print(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2224d21",
   "metadata": {},
   "source": [
    "### 3.2 Transition probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "627ce832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start state:\n",
      "┌───┬───┐\n",
      "│   │   │\n",
      "├───┼───┤\n",
      "│   │   │\n",
      "└───┴───┘\n",
      "\n",
      "New state:\n",
      "┌───┬───┐ ┌───┬───┐ ┌───┬───┐ ┌───┬───┐\n",
      "│ Y │   │ │   │   │ │ Y │   │ │   │ Y │\n",
      "├───┼───┤ ├───┼───┤ ├───┼───┤ ├───┼───┤\n",
      "│ R │   │ │ R │ Y │ │ R │   │ │ R │   │\n",
      "└───┴───┘ └───┴───┘ └───┴───┘ └───┴───┘\n",
      " Action 0  Action 0  Action 0  Action 0\n",
      " p: 0.5    p: 0.5    p: 0.5    p: 0.0  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = FourInARowEnv(\n",
    "  yellow_agent  = FourInARowRandomAgent,\n",
    "  width         = 2,\n",
    "  height        = 2,\n",
    "  win_condition = 2,\n",
    "  first_turn    = Players.RED\n",
    ")\n",
    "s0 = deepcopy(env._state)\n",
    "s1 = deepcopy(s0)\n",
    "s1._grid = [\n",
    "  [BoxState.RED   , BoxState.YELLOW],\n",
    "  [BoxState.EMPTY , BoxState.EMPTY ]\n",
    "]\n",
    "s2 = deepcopy(s0)\n",
    "s2._grid = [\n",
    "  [BoxState.RED   , BoxState.EMPTY],\n",
    "  [BoxState.YELLOW, BoxState.EMPTY]\n",
    "]\n",
    "s3 = deepcopy(s0)\n",
    "s3._grid = [\n",
    "  [BoxState.RED    , BoxState.YELLOW],\n",
    "  [BoxState.EMPTY  , BoxState.EMPTY ]\n",
    "]\n",
    "s4 = deepcopy(s0)\n",
    "s4._grid = [\n",
    "  [BoxState.RED    , BoxState.EMPTY ],\n",
    "  [BoxState.EMPTY  , BoxState.YELLOW]\n",
    "]\n",
    "\n",
    "print('Start state:')\n",
    "print(FourInARowRenderer(s0).render())\n",
    "\n",
    "display = render_multiple_states(\n",
    "  states          = [s1, s2, s3, s4],\n",
    "  columns         = 8,\n",
    "  additional_info = lambda s: f'Action 0\\np: {round(env.get_transition_prob(0, s, s0), 2)}'\n",
    ")\n",
    "\n",
    "print('\\nNew state:')\n",
    "print(display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f161cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start state:\n",
      "┌───┬───┐\n",
      "│   │   │\n",
      "├───┼───┤\n",
      "│ R │ Y │\n",
      "└───┴───┘\n",
      "\n",
      "New state:\n",
      "┌───┬───┐ ┌───┬───┐\n",
      "│ R │   │ │ R │ Y │\n",
      "├───┼───┤ ├───┼───┤\n",
      "│ R │ Y │ │ R │ Y │\n",
      "└───┴───┘ └───┴───┘\n",
      " Action 0  Action 0\n",
      " p: 1.0    p: 0.0  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = FourInARowEnv(\n",
    "  yellow_agent  = FourInARowRandomAgent,\n",
    "  width         = 2,\n",
    "  height        = 2,\n",
    "  win_condition = 2,\n",
    "  first_turn    = Players.RED\n",
    ")\n",
    "s0 = deepcopy(env._state)\n",
    "s0._grid = [\n",
    "  [BoxState.RED   , BoxState.EMPTY ],\n",
    "  [BoxState.YELLOW, BoxState.EMPTY ]\n",
    "]\n",
    "s1 = deepcopy(s0)\n",
    "s1._grid = [\n",
    "  [BoxState.RED   , BoxState.RED],\n",
    "  [BoxState.YELLOW, BoxState.EMPTY ]\n",
    "]\n",
    "s2 = deepcopy(s0)\n",
    "s2._grid = [\n",
    "  [BoxState.RED   , BoxState.RED   ],\n",
    "  [BoxState.YELLOW, BoxState.YELLOW]\n",
    "]\n",
    "\n",
    "print('Start state:')\n",
    "print(FourInARowRenderer(s0).render())\n",
    "\n",
    "display = render_multiple_states(\n",
    "  states          = [s1, s2],\n",
    "  columns         = 8,\n",
    "  additional_info = lambda s: f'Action 0\\np: {round(env.get_transition_prob(0, s, s0), 2)}'\n",
    ")\n",
    "\n",
    "print('\\nNew state:')\n",
    "print(display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3b61a",
   "metadata": {},
   "source": [
    "### 3.3 Rewards\n",
    "\n",
    "The reward model is very basic. If the player wins the reward will result in 1 and if the opponent wins it results in a -1. For neutral states it will return 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4399d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for player red:\n",
      "┌───┬───┐ ┌───┬───┐ ┌───┬───┐\n",
      "│   │   │ │ R │   │ │   │ Y │\n",
      "├───┼───┤ ├───┼───┤ ├───┼───┤\n",
      "│ R │ Y │ │ R │ Y │ │ R │ Y │\n",
      "└───┴───┘ └───┴───┘ └───┴───┘\n",
      " r: 0      r: 1      r: -1   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = FourInARowEnv(\n",
    "  yellow_agent  = FourInARowRandomAgent,\n",
    "  width         = 2,\n",
    "  height        = 2,\n",
    "  win_condition = 2,\n",
    "  first_turn    = Players.RED\n",
    ")\n",
    "s0 = deepcopy(env._state)\n",
    "s0._grid = [\n",
    "  [BoxState.RED   , BoxState.EMPTY ],\n",
    "  [BoxState.YELLOW, BoxState.EMPTY ]\n",
    "]\n",
    "s1 = deepcopy(s0)\n",
    "s1._grid = [\n",
    "  [BoxState.RED   , BoxState.RED],\n",
    "  [BoxState.YELLOW, BoxState.EMPTY ]\n",
    "]\n",
    "s2 = deepcopy(s0)\n",
    "s2._grid = [\n",
    "  [BoxState.RED   , BoxState.EMPTY  ],\n",
    "  [BoxState.YELLOW, BoxState.YELLOW ]\n",
    "]\n",
    "\n",
    "display = render_multiple_states(\n",
    "  states          = [s0, s1, s2],\n",
    "  columns         = 8,\n",
    "  additional_info = lambda s: f'r: {env.get_reward_for_state(s, Players.RED)}'\n",
    ")\n",
    "print('Rewards for player red:')\n",
    "print(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d6e65",
   "metadata": {},
   "source": [
    "## 4 Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ff9b4",
   "metadata": {},
   "source": [
    "### 4.1 Random\n",
    "This agent will simply choose a valid action at random regardless of state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833eb528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐\n",
      "│   │   │   │ │   │   │   │ │   │   │ Y │ │ Y │   │ Y │ │ Y │   │ Y │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│   │   │   │ │   │   │   │ │   │   │ R │ │ R │   │ R │ │ R │ Y │ R │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│   │   │   │ │ R │   │ Y │ │ R │   │ Y │ │ R │   │ Y │ │ R │ R │ Y │\n",
      "└───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘\n",
      "                                                                     \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = FourInARowEnv(\n",
    "  yellow_agent  = FourInARowRandomAgent,\n",
    "  width         = 3,\n",
    "  height        = 3,\n",
    "  win_condition = 3,\n",
    "  first_turn    = Players.RED\n",
    ")\n",
    "\n",
    "agent = FourInARowRandomAgent(env)\n",
    "\n",
    "states = []\n",
    "states.append(deepcopy(env.get_state()))\n",
    "while not env.is_done():\n",
    "  env.step(agent.get_move())\n",
    "  states.append(deepcopy(env.get_state()))\n",
    "\n",
    "display = render_multiple_states(\n",
    "  states = states,\n",
    "  columns = 8,\n",
    "  additional_info = lambda _: ''\n",
    ")\n",
    "print(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d21b04",
   "metadata": {},
   "source": [
    "### 4.2 Semi-random\n",
    "This agent's policy is almost identical to the random agent's policy with one exception: it will look 1 step ahead. It will score if it is only one step away from winning, similarly it will block the opponent if they are one step away from winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24bdf729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐\n",
      "│   │   │   │ │   │   │   │ │ Y │   │   │ │ Y │   │   │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│   │   │   │ │ Y │   │   │ │ Y │   │   │ │ Y │   │   │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│   │   │   │ │ R │   │   │ │ R │   │ R │ │ R │ R │ R │\n",
      "└───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘\n",
      "                                                       \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = FourInARowEnv(\n",
    "  yellow_agent  = FourInARowRandomAgent,\n",
    "  width         = 3,\n",
    "  height        = 3,\n",
    "  win_condition = 3,\n",
    "  first_turn    = Players.RED\n",
    ")\n",
    "\n",
    "agent = FourInARowSemiRandomAgent(env)\n",
    "\n",
    "states = []\n",
    "states.append(deepcopy(env.get_state()))\n",
    "while not env.is_done():\n",
    "  env.step(agent.get_move())\n",
    "  states.append(deepcopy(env.get_state()))\n",
    "\n",
    "display = render_multiple_states(\n",
    "  states = states,\n",
    "  columns = 8,\n",
    "  additional_info = lambda _: ''\n",
    ")\n",
    "print(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced42e1",
   "metadata": {},
   "source": [
    "### 4.3 Brute-force\n",
    "\n",
    "The brute-force agent will walk through all possibilities when making a move and returns the action that yields the highest reward. This will result in the best results but will not work for environments that could go on forever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b58ae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐\n",
      "│   │   │   │ │   │   │   │ │ Y │   │   │ │ Y │   │   │ │ Y │   │   │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│   │   │   │ │   │   │   │ │ R │   │   │ │ R │   │ R │ │ R │ R │ R │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│   │   │   │ │ R │   │ Y │ │ R │   │ Y │ │ R │ Y │ Y │ │ R │ Y │ Y │\n",
      "└───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘\n",
      "                                                                     \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = FourInARowEnv(\n",
    "  yellow_agent  = FourInARowRandomAgent,\n",
    "  width         = 3,\n",
    "  height        = 3,\n",
    "  win_condition = 3,\n",
    "  first_turn    = Players.RED\n",
    ")\n",
    "\n",
    "agent = FourInARowMinMaxAgent(env)\n",
    "\n",
    "states = []\n",
    "states.append(deepcopy(env.get_state()))\n",
    "while not env.is_done():\n",
    "  env.step(agent.get_move())\n",
    "  states.append(deepcopy(env.get_state()))\n",
    "\n",
    "display = render_multiple_states(\n",
    "  states = states,\n",
    "  columns = 8,\n",
    "  additional_info = lambda _: ''\n",
    ")\n",
    "print(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e645f33c",
   "metadata": {},
   "source": [
    "### 4.4 Value Iteration\n",
    "\n",
    "The value iteration agent uses value iteration to solve Bellman's equation. It will compute the utility for every state at creation and will use it in the future to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7fe6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐\n",
      "│   │   │   │ │   │   │   │ │   │   │   │ │   │   │   │ │   │ R │ Y │ │ R │ R │ Y │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│   │   │   │ │   │   │   │ │   │ R │ Y │ │ Y │ R │ Y │ │ Y │ R │ Y │ │ Y │ R │ Y │\n",
      "├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤\n",
      "│   │   │   │ │   │ Y │ R │ │   │ Y │ R │ │ R │ Y │ R │ │ R │ Y │ R │ │ R │ Y │ R │\n",
      "└───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘\n",
      "                                                                                   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = FourInARowEnv(\n",
    "  yellow_agent  = FourInARowRandomAgent,\n",
    "  width         = 3,\n",
    "  height        = 3,\n",
    "  win_condition = 3,\n",
    "  first_turn    = Players.RED\n",
    ")\n",
    "\n",
    "agent = FourInARowValueIterationAgent(env)\n",
    "\n",
    "states = []\n",
    "states.append(deepcopy(env.get_state()))\n",
    "while not env.is_done():\n",
    "  env.step(agent.get_move())\n",
    "  states.append(deepcopy(env.get_state()))\n",
    "\n",
    "display = render_multiple_states(\n",
    "  states = states,\n",
    "  columns = 8,\n",
    "  additional_info = lambda _: ''\n",
    ")\n",
    "print(display)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1932237d5f6e1407a57ef8cc07d4a91480fed45ce28e9b3cad897d61089d0e1"
  },
  "kernelspec": {
   "display_name": "PyCharm (DecisionTheory)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "%load_ext autoreload\n",
     "%autoreload 2\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
