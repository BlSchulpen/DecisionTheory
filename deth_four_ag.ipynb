{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6896d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfe616",
   "metadata": {},
   "source": [
    "# Four in a row Agent and Environment\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vestibulum, nibh sit amet convallis varius, neque arcu luctus nunc, gravida sollicitudin quam nisi mattis ante. Duis imperdiet lorem a pharetra iaculis. In hac habitasse platea dictumst. Vivamus nisl ex, semper eget facilisis vel, varius ac est. Curabitur eget nisl erat. In egestas molestie mi, a pulvinar ante sodales sit amet. Vivamus interdum sem eu interdum placerat. Duis tempus purus non lectus pharetra, ac gravida mauris ultricies. Vestibulum condimentum ut dolor nec interdum. Curabitur et ex magna. In erat purus, laoreet vitae elementum sed, vehicula eu lorem. Nulla maximus eu est vel tempor. Ut et aliquam quam, eu pellentesque leo. Ut at suscipit eros. Phasellus luctus, justo nec molestie pharetra, massa sem hendrerit arcu, ut ullamcorper justo augue et est. Donec dictum sem id tristique pharetra.\n",
    "\n",
    "#TODO state which libaries are used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811a4e2",
   "metadata": {},
   "source": [
    "## 1. Definition of the Environment\n",
    "\n",
    "- States: The environment is in one of the following states $S = (S_0, S_1, ..., S_w)$ with $S_x = (S_{x0}, S_{x1}, ..., S_{xh})$ and $S_{xy} \\in \\{E, R, Y\\}$.\n",
    "- Actions: The set of  available actions: $a \\in \\{n \\in \\mathbb{N}: (|E \\in S_n| > 0)\\}$ (the agent can place their chip in any non-full column).\n",
    "- Transitions: The transition depends on the opponent's strategy. If we assume the opponent plays randomly then the probability of them picking a viable column is $\\dfrac{1}{nr\\;of\\;valid\\;columns}$\n",
    "\n",
    "The `FourInARowEnv` class has the following methods:\n",
    "- `reset()` resets the environment's state to it's initial state.\n",
    "- `step(action)` processes the action of the agent.\n",
    "- `render()` displays the state using box characters.\n",
    "\n",
    "To allow an agent to calculate optimal decisions using model information, these methods are also available:\n",
    "- `get_possible_states()` calculates all possible future states.\n",
    "- `is_done()` checks if there is a winner of if the board is full.\n",
    "- `get_reward_for_new(state)` simplified version $R(s)$ of the general reward function: $R(s, a, s')$.\n",
    "- `get_transition_prob(action, new_state, old_state)` $P(s' \\mid s, a)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec035b",
   "metadata": {},
   "source": [
    "## 2. Environment Creation\n",
    "\n",
    "We are able to create an environment with several parameters:\n",
    "\n",
    "- `yellow_agent` is used to define an agent that will act as the opponent.\n",
    "- `width` the width of the playing field.\n",
    "- `height` the height of the playing field.\n",
    "- `win_conditions` the number of chips in a row required to win.\n",
    "- `first_turn` the player that will play first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a12f32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import FourInARowEnv, FourInARowRandomAgent, Players, FourInARowRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e3c6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = FourInARowEnv(\n",
    "  yellow_agent  = FourInARowRandomAgent,\n",
    "  width         = 3,\n",
    "  height        = 3,\n",
    "  win_condition = 4,\n",
    "  first_turn    = Players.RED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987af686",
   "metadata": {},
   "source": [
    "### 2.1 Displaying the board\n",
    "\n",
    "The board is displayed using box characters. The letter `Y` represents a yellow chip, and the letter `R` represents a red one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "260dddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐\n",
      "│   │   │   │\n",
      "├───┼───┼───┤\n",
      "│   │   │ Y │\n",
      "├───┼───┼───┤\n",
      "│   │   │ R │\n",
      "└───┴───┴───┘\n"
     ]
    }
   ],
   "source": [
    "red_agent = FourInARowRandomAgent(environment)\n",
    "\n",
    "environment.step(red_agent.get_move())\n",
    "\n",
    "print(environment.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb13f0",
   "metadata": {},
   "source": [
    "### 2.2 Possible states and actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a17961dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "┌───┬───┬───┐\n",
      "│   │   │   │\n",
      "├───┼───┼───┤\n",
      "│   │   │   │\n",
      "├───┼───┼───┤\n",
      "│ R │   │   │\n",
      "└───┴───┴───┘\n",
      "┌───┬───┬───┐\n",
      "│   │   │   │\n",
      "├───┼───┼───┤\n",
      "│ Y │   │   │\n",
      "├───┼───┼───┤\n",
      "│ R │   │   │\n",
      "└───┴───┴───┘\n",
      "┌───┬───┬───┐\n",
      "│ R │   │   │\n",
      "├───┼───┼───┤\n",
      "│ Y │   │   │\n",
      "├───┼───┼───┤\n",
      "│ R │   │   │\n",
      "└───┴───┴───┘\n"
     ]
    }
   ],
   "source": [
    "environment.reset()\n",
    "\n",
    "print(environment.get_possible_actions())\n",
    "for state in environment.get_possible_states()[0:3]:\n",
    "  print(FourInARowRenderer(state).render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d6e65",
   "metadata": {},
   "source": [
    "# 4. Value Iteration\n",
    "\n",
    "Value Iteration is based on the Bellman update:\n",
    "\n",
    "(6) $U_{i+1}(s) = \\underset{a}{max} \\sum_{s'} P(s' \\mid s, a) \\space [ R(s, a, s') + \\gamma \\space U_i(s') ]$\n",
    "\n",
    "Using equation (3) this simplifies to:\n",
    "\n",
    "(7) $U_{i+1}(s) = \\underset{a}{max} \\space Q_i(s, a)$\n",
    "\n",
    "One can prove that after enough iterations $U_{i+1}(s) \\approx U(s)$, after which Bellman's equation is satisfied.  \n",
    "Since there is only one solution to Bellman's equation, it does not matter with which $U_0(s)$ you start!\n",
    "\n",
    "The algorithm below is Value Iteration with one simplification: $\\gamma$ the so-called discount factor, is set to 1.\n",
    "\n",
    "\n",
    "help...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b7fe6e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<src.FourInARowState.FourInARowState object at 0x000002E21D5F0880>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17048/44114805.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mValueIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17048/44114805.py\u001b[0m in \u001b[0;36mValueIteration\u001b[1;34m(mdp, error)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_possible_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;31m#print_U(U)  # to illustrate the iteration process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: <src.FourInARowState.FourInARowState object at 0x000002E21D5F0880>"
     ]
    }
   ],
   "source": [
    "def get_initial_U(mdp):\n",
    "    U = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        U[s] = mdp.get_reward_for_new(s)\n",
    "    return U\n",
    "    \n",
    "def Q_Value(mdp, s, a, U):\n",
    "    Q = 0.0\n",
    "    for s_p in mdp.get_possible_states():\n",
    "        P = mdp.get_transition_prob(a, s_p, s)\n",
    "        R = mdp.get_reward_for_new(s_p)\n",
    "        Q += P * (R + U[tuple(s_p)])\n",
    "    return Q\n",
    "\n",
    "def ValueIteration(mdp, error=0.00001):\n",
    "    # from AIMA 4th edition without discount gamma \n",
    "    U_p = get_initial_U(mdp) # U_p = U'\n",
    "    delta = float('inf')\n",
    "    while delta > error:\n",
    "        U = {}\n",
    "        for s in mdp.get_possible_states():\n",
    "            U[s] = U_p[s]\n",
    "        #print_U(U)  # to illustrate the iteration process\n",
    "        delta = 0\n",
    "        for s in mdp.get_possible_states():\n",
    "            max_a = float('-inf')\n",
    "            for a in mdp.get_possible_actions(s):\n",
    "                q = Q_Value(mdp, s, a, U) \n",
    "                if q > max_a:\n",
    "                    max_a = q\n",
    "            U_p[s] = max_a\n",
    "            if abs(U_p[s] - U[s]) > delta:\n",
    "                delta = abs(U_p[s] - U[s])\n",
    "    return U\n",
    "\n",
    "ValueIteration(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ce832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(environment.is_done())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1932237d5f6e1407a57ef8cc07d4a91480fed45ce28e9b3cad897d61089d0e1"
  },
  "kernelspec": {
   "display_name": "PyCharm (DecisionTheory)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "%load_ext autoreload\n",
     "%autoreload 2\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
