{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6896d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfe616",
   "metadata": {},
   "source": [
    "# Four in a row Agent and Environment\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vestibulum, nibh sit amet convallis varius, neque arcu luctus nunc, gravida sollicitudin quam nisi mattis ante. Duis imperdiet lorem a pharetra iaculis. In hac habitasse platea dictumst. Vivamus nisl ex, semper eget facilisis vel, varius ac est. Curabitur eget nisl erat. In egestas molestie mi, a pulvinar ante sodales sit amet. Vivamus interdum sem eu interdum placerat. Duis tempus purus non lectus pharetra, ac gravida mauris ultricies. Vestibulum condimentum ut dolor nec interdum. Curabitur et ex magna. In erat purus, laoreet vitae elementum sed, vehicula eu lorem. Nulla maximus eu est vel tempor. Ut et aliquam quam, eu pellentesque leo. Ut at suscipit eros. Phasellus luctus, justo nec molestie pharetra, massa sem hendrerit arcu, ut ullamcorper justo augue et est. Donec dictum sem id tristique pharetra.\n",
    "\n",
    "#TODO state which libaries are used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811a4e2",
   "metadata": {},
   "source": [
    "## 1. Definition of the Environment\n",
    "\n",
    "- States: The environment is in one of the following states $S = (S_0, S_1, ..., S_w)$ with $S_x = (S_{x0}, S_{x1}, ..., S_{xh})$ and $S_{xy} \\in \\{E, R, Y\\}$.\n",
    "- Actions: The set of  available actions: $a \\in \\{n \\in \\mathbb{N}: (|E \\in S_n| > 0)\\}$ (the agent can place their chip in any non-full column).\n",
    "- Transitions: The transition depends on the opponent's strategy. If we assume the opponent plays randomly then the probability of them picking a viable column is $\\dfrac{1}{nr\\;of\\;valid\\;columns}$\n",
    "\n",
    "The `FourInARowEnv` class has the following methods:\n",
    "- `reset()` resets the environment's state to it's initial state.\n",
    "- `step(action)` processes the action of the agent.\n",
    "- `render()` displays the state using box characters.\n",
    "\n",
    "To allow an agent to calculate optimal decisions using model information, these methods are also available:\n",
    "- `get_possible_states()` calculates all possible future states.\n",
    "- `is_done()` checks if there is a winner of if the board is full.\n",
    "- `get_reward_for_new(state)` simplified version $R(s)$ of the general reward function: $R(s, a, s')$.\n",
    "- `get_transition_prob(action, new_state, old_state)` $P(s' \\mid s, a)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec035b",
   "metadata": {},
   "source": [
    "## 2. Environment Creation\n",
    "\n",
    "We are able to create an environment with several parameters:\n",
    "\n",
    "- `yellow_agent` is used to define an agent that will act as the opponent.\n",
    "- `width` the width of the playing field.\n",
    "- `height` the height of the playing field.\n",
    "- `win_conditions` the number of chips in a row required to win.\n",
    "- `first_turn` the player that will play first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a12f32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import FourInARowEnv, FourInARowRandomAgent, Players, FourInARowRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e3c6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = FourInARowEnv(\n",
    "  yellow_agent  = FourInARowRandomAgent,\n",
    "  width         = 3,\n",
    "  height        = 3,\n",
    "  win_condition = 4,\n",
    "  first_turn    = Players.RED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987af686",
   "metadata": {},
   "source": [
    "### 2.1 Displaying the board\n",
    "\n",
    "The board is displayed using box characters. The letter `Y` represents a yellow chip, and the letter `R` represents a red one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "260dddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐\n",
      "│   │   │   │\n",
      "├───┼───┼───┤\n",
      "│   │   │ Y │\n",
      "├───┼───┼───┤\n",
      "│   │   │ R │\n",
      "└───┴───┴───┘\n"
     ]
    }
   ],
   "source": [
    "red_agent = FourInARowRandomAgent(environment)\n",
    "\n",
    "environment.step(red_agent.get_move())\n",
    "\n",
    "print(environment.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb13f0",
   "metadata": {},
   "source": [
    "### 2.2 Possible states and actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a17961dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "┌───┬───┬───┐\n",
      "│   │   │   │\n",
      "├───┼───┼───┤\n",
      "│   │   │   │\n",
      "├───┼───┼───┤\n",
      "│ R │   │   │\n",
      "└───┴───┴───┘\n",
      "┌───┬───┬───┐\n",
      "│   │   │   │\n",
      "├───┼───┼───┤\n",
      "│ Y │   │   │\n",
      "├───┼───┼───┤\n",
      "│ R │   │   │\n",
      "└───┴───┴───┘\n",
      "┌───┬───┬───┐\n",
      "│ R │   │   │\n",
      "├───┼───┼───┤\n",
      "│ Y │   │   │\n",
      "├───┼───┼───┤\n",
      "│ R │   │   │\n",
      "└───┴───┴───┘\n"
     ]
    }
   ],
   "source": [
    "environment.reset()\n",
    "\n",
    "print(environment.get_possible_actions())\n",
    "for state in environment.get_possible_states()[0:3]:\n",
    "  print(FourInARowRenderer(state).render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "627ce832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(environment.is_done())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1932237d5f6e1407a57ef8cc07d4a91480fed45ce28e9b3cad897d61089d0e1"
  },
  "kernelspec": {
   "display_name": "PyCharm (DecisionTheory)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "%load_ext autoreload\n",
     "%autoreload 2\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
